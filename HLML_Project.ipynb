{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d7197f",
   "metadata": {},
   "source": [
    "# HLML: Project\n",
    "Dennis Agafonov (12528269)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa191fc",
   "metadata": {},
   "source": [
    "This code is the combined work of Dennis Agafonov and Jelke Matthijsse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6b81d",
   "metadata": {},
   "source": [
    "Import all necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079af08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# sklearn imports for classification\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For query-by-committee train different models\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1310436",
   "metadata": {},
   "source": [
    "Load a subset from the MNIST dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of the MNIST dataset from sklearn\n",
    "digits = load_digits()\n",
    "# 60-20-20 split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(digits.data, digits.target, test_size=0.40, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477155bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(x, y):\n",
    "    '''\n",
    "    Function that computes the accuracy score of a trained model given data\n",
    "    \n",
    "    :param x: features of datapoint\n",
    "    :param y: corresponding labels of datapoints\n",
    "    \n",
    "    :return: accuracy of trained regression model on given data, and trained model.\n",
    "    '''\n",
    "    # initialize and fit logistic regression on data\n",
    "    target_clf = LogisticRegression(solver='saga',max_iter=10000, random_state=42)\n",
    "    target_clf.fit(x, y)\n",
    "    \n",
    "    # predict new labels\n",
    "    y_pred = target_clf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    return acc, target_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3434548",
   "metadata": {},
   "source": [
    "Computing the acquisition score for each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy_score(point, model):\n",
    "    '''\n",
    "    Function that computes the entropy score of a datapoint\n",
    "    '''\n",
    "    # get prob and log prob of point\n",
    "    x = point[0].reshape(1, -1)\n",
    "    log_prob = model.predict_log_proba(x)[0]\n",
    "    prob = model.predict_proba(x)[0]\n",
    "        \n",
    "    # calculate entropy\n",
    "    entropy = -1 * np.sum(np.multiply(prob, log_prob))\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "def get_qbc_score(point, model1, model2, model3, model4):\n",
    "    '''\n",
    "    Function that computes the query-by-committee score of a datapoint\n",
    "    '''\n",
    "    QBC = 0\n",
    "    \n",
    "    # get prediction for datapoint for every model\n",
    "    x = point[0].reshape(1, -1)\n",
    "    pred_1 = model1.predict(x)\n",
    "    pred_2 = model2.predict(x)\n",
    "    pred_3 = model3.predict(x)\n",
    "    pred_4 = model4.predict(x)\n",
    "    models = [pred_1, pred_2, pred_3, pred_4]\n",
    "    \n",
    "    # calculate qbc as number of disagreements between models\n",
    "    for comb in combinations(models, 2):\n",
    "        if comb[0] != comb[1]:\n",
    "            QBC +=1\n",
    "        \n",
    "    return QBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beabb2c",
   "metadata": {},
   "source": [
    "These provide that are necessary to compute the Query-by-Committee acquisition score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(dataset):\n",
    "\n",
    "    '''\n",
    "    Function that retrains the different committee of models\n",
    "    '''\n",
    "    # unzip the current dataset into x and y\n",
    "    X_train, y_train = zip(*dataset)\n",
    "    \n",
    "    # fit support-vector-machine\n",
    "    model1 = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel ='rbf', random_state=42))\n",
    "    model1.fit(X_train, y_train)\n",
    "\n",
    "    # fit naive bayes classifier\n",
    "    model2 = GaussianNB()\n",
    "    model2.fit(X_train, y_train)                                             \n",
    "\n",
    "    # fit decision-tree model\n",
    "    model3 = DecisionTreeClassifier(random_state=42)\n",
    "    model3.fit(X_train, y_train)\n",
    "\n",
    "    # fit logistic regression model\n",
    "    model4 = LogisticRegression(solver='saga',max_iter=10000, random_state=42)\n",
    "    model4.fit(X_train, y_train)\n",
    "    \n",
    "    return model1, model2, model3, model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f60017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datapoints(pool, curr_model, acquisition, batch_size, model1, model2, model3, model4):\n",
    "    '''\n",
    "    Function that returns the datapoints with the highest uncertainty\n",
    "    \n",
    "    :param pool: current pool set from which datapoint can be chosen\n",
    "    :param curr_model: current model trained on dataset from previous iteration (used to obtain pred for candidate datapoint)\n",
    "    :param acquisition: str that says which acquisition function (uncertainty sampling or QBC) to use\n",
    "    :param batch_size: number of datapoints that are extracted \n",
    "    \n",
    "    :return: batch_size number of datapoints, and the updated poolset\n",
    "    '''\n",
    "    uncertainty_per_point = {}\n",
    "    \n",
    "    # loop through all points in pool \n",
    "    for i, point in enumerate(pool):\n",
    "        \n",
    "        if acquisition == 'entropy':\n",
    "            score = get_entropy_score(point, curr_model)\n",
    "        elif acquisition == 'qbc':\n",
    "            score = get_qbc_score(point, model1, model2, model3, model4)\n",
    "        \n",
    "        # add index of point (key) and corresponding entropy (value) to dct\n",
    "        uncertainty_per_point[i] = score\n",
    "        \n",
    "    # obtain index of point that has highest entropy\n",
    "    max_point_idx = sorted(uncertainty_per_point, key=lambda x: uncertainty_per_point[x], reverse=True)[:batch_size]\n",
    "    \n",
    "    # corresponding datapoint of idx\n",
    "    datapoints = [pool[i] for i in max_point_idx]\n",
    "    \n",
    "    for index in sorted(max_point_idx, reverse=True):\n",
    "        pool.pop(index)\n",
    "    \n",
    "    return datapoints, pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_learning(acquisition, batch_size, nr_labeled, retrain_it):\n",
    "    train = list(zip(X_train, y_train))\n",
    "    current_dataset = train[:nr_labeled]\n",
    "    pool_set = train[nr_labeled:]\n",
    "    \n",
    "    model1, model2, model3, model4 = fit_models(current_dataset)\n",
    "    val_accuracies = []\n",
    "    moving_avg_window = 7  # window size for moving average\n",
    "    moving_avg_threshold = 0.0001  # threshold for change in moving average\n",
    "    prev_moving_avg = 0  # initialize previous moving average\n",
    "    i = 0\n",
    "    while pool_set:\n",
    "        current_X, current_y = zip(*current_dataset)\n",
    "        val_acc, current_clf = compute_acc(current_X, current_y)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Compute moving average\n",
    "        if len(val_accuracies) >= moving_avg_window:\n",
    "            current_moving_avg = sum(val_accuracies[-moving_avg_window:]) / moving_avg_window\n",
    "            if abs(current_moving_avg - prev_moving_avg) < moving_avg_threshold:\n",
    "                print('Validation accuracy plateaued; stopping!')\n",
    "                break\n",
    "            prev_moving_avg = current_moving_avg\n",
    "\n",
    "        new_points, pool_set = get_datapoints(pool_set, current_clf, acquisition, batch_size, model1, model2, model3, model4)\n",
    "        current_dataset = current_dataset + new_points\n",
    "\n",
    "        if (i+1) % retrain_it == 0 and acquisition == 'qbc':\n",
    "            model1, model2, model3, model4 = fit_models(current_dataset)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if len(pool_set) == 0:\n",
    "        print('Ran out of points in pool set; stopping!')\n",
    "\n",
    "    return val_accuracies, current_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs_qbc = []\n",
    "test_accs_qbc = []\n",
    "val_accs_ent = []\n",
    "test_accs_ent = []\n",
    "batch_sizes = [1,5,10,15,20,50,100]\n",
    "for batch_size in batch_sizes:\n",
    "    val_acc_qbc, final_model_qbc = active_learning('qbc', batch_size, 40, 5)\n",
    "    test_score_qbc = final_model_qbc.score(X_test, y_test)\n",
    "    val_acc_ent, final_model_ent = active_learning('entropy', batch_size, 40, 5)\n",
    "    test_score_ent = final_model_ent.score(X_test, y_test)\n",
    "    val_accs_qbc.append(val_acc_qbc)\n",
    "    val_accs_ent.append(val_acc_ent)\n",
    "    test_accs_qbc.append(test_score_qbc)\n",
    "    test_accs_ent.append(test_score_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12, 12))\n",
    "\n",
    "for sublist, batch_size in zip(val_accs_qbc, batch_sizes):\n",
    "    ax1.plot(sublist, label=f\"Batch size {batch_size}\",  marker='o')\n",
    "ax1.set_title(\"Validation accuracy for each batch size (AF: QBC)\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.legend()\n",
    "\n",
    "for sublist, batch_size in zip(val_accs_ent, batch_sizes):\n",
    "    ax2.plot(sublist, label=f\"Batch size {batch_size}\",  marker='o')\n",
    "ax2.set_title(\"Validation accuracy for each batch size (AF: US)\")\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab62727",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accs_qbc)\n",
    "print(test_accs_ent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
